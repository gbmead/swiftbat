#the following code is from the parsotat/BatAnalysis/notebooks/trial_detection_Crab.ipynb, with some modifications for desired use
#the ultimate goal is to graph long term variability of x-ray peaked blazar to best constrain mechanisms for relativistic jet particle acceleration

#begin the analysis: important to activate bat ("conda activate bat"), import heasoftpy ("heainit"), and open data folder (example, "cd data") before runnning 

import glob
import os
import sys
import batanalysis as ba
import matplotlib.pyplot as plt
import numpy as np
from astropy.time import Time, TimeDelta
from astropy.io import fits
from pathlib import Path
import swiftbat
import swiftbat.swutil as sbu
import pickle

plt.ion()

#set the path with the BAT survey data, specifcying "user", a new folder for your object (in this case quasar 3C 273, other option, blazar Mrk421 
#query heasarc for all the data within the time period of interest and download it (note, including more than one or two years can produce error as heasarc times out) 
  
newdir = Path("/home/user/data/bat/3C273data")
ba.datadir(newdir, mkdir=True)
object_name='3C273'
queryargs = dict(time="2008-10-27 .. 2009-10-27", fields='All', resultmax=0)

#use swiftbat to create a bat source object
object_location = swiftbat.simbadlocation("3C273")
object_batsource = swiftbat.source(ra=object_location[0], dec=object_location[1], name=object_name)
table_everything = ba.from_heasarc(name=None, **queryargs)
minexposure = 2000     # cm^2 after cos adjust

#calculate the exposure with partial coding
exposures = np.array([object_batsource.exposure(ra=row['RA'], dec=row['DEC'], roll=row['ROLL_ANGLE'])[0] for row in table_everything])

#select the observations that have greater than the minimum desired exposure
table_exposed = table_everything[exposures > minexposure]
print(f"Finding everything finds {len(table_everything)} observations, of which {len(table_exposed)} have more than {minexposure:0} cm^2 coded")

#download the data, specify the quantity of the data downloaded, you may check the length of the table (via table_exposed(len))
#issues may occur the longer runtime and pointing ids, success for at least 100 rows 

result = ba.download_swiftdata(table_exposed[:100],save_dir="/home/bmead/data/bat/3C273data",reload='True')

#infrequent issue the run skips a row, if needed, specifiy and redownload 
#result1 =  ba.download_swiftdata(table_exposed[3],save_dir="/home/bmead/data/bat/3C273data",reload='True')
#get a list of the fully downloaded observation IDs
#obs_ids=[i for i in table_exposed[:5]['OBSID'] if result[i]['success']or result1['success']]

obs_ids=[i for i in table_exposed[:100]['OBSID'] if result[i]['success']]

#create a path for the pattern maps, thus the mosaic anaylsis can correctly filter background noise 
input_dict=dict(cleansnr=6,cleanexpr='ALWAYS_CLEAN==T')
noise_map_dir=Path("/Users/bmead/data/bat/PATTERN_MAPS/")
batsurvey_obs=ba.parallel.batsurvey_analysis(obs_ids, input_dict=input_dict, patt_noise_dir=noise_map_dir, nprocs=20)

#preview the snapshot pointing values of rate, snr, and the fitted flux and photon index
batsurvey_obs=ba.parallel.batspectrum_analysis(batsurvey_obs, object_name, ul_pl_index=2.15, recalc=True,nprocs=14)
fig, axes=ba.plot_survey_lc(batsurvey_obs, id_list=object_name, time_unit="UTC", values=["rate","snr", "flux", "PhoIndex", "exposure"], calc_lc=True)

#mosaicing analysis: group together all the BAT survey observations that we want to include in this step in our analysis
#information should load as follows, outventory_file=Path("/bat/3C273data/mosaiced_surveyresults/grouped_outventory/")
outventory_file=ba.merge_outventory(batsurvey_obs)

#to define the time bins for which we will create mosaic images and analyze them, to test our code, we will use the same 1 month binning as the 22 month survey paper used
#remember to adjust the "Time" to desired end time, no need to specify start time 
time_bins=ba.group_outventory(outventory_file, np.timedelta64(1, "M"), end_datetime=Time("2009-10-27"), recalc=True)

mosaic_list, total_mosaic=ba.parallel.batmosaic_analysis(batsurvey_obs, outventory_file, time_bins, nprocs=8)

mosaic_list=ba.parallel.batspectrum_analysis(mosaic_list, object_name, ul_pl_index=2.15, nprocs=11)
total_mosaic=ba.parallel.batspectrum_analysis(total_mosaic, object_name, ul_pl_index=2.15, use_cstat=False, nprocs=1)
fig, axes=ba.plot_survey_lc(mosaic_list, id_list=object_name, time_unit="UTC", values=["rate","snr", "flux", "PhoIndex", "exposure"])
fig, axes=ba.plot_survey_lc([batsurvey_obs,mosaic_list], id_list=object_name, time_unit="UTC", values=["rate","snr", "flux", "PhoIndex", "exposure"], same_figure=True)

#weekly mosaic (weekly averages) 
outventory_file_weekly=ba.merge_outventory(batsurvey_obs, savedir=Path('./3C273data/weekly_mosaiced_surveyresults/', reload=True))
time_bins_weekly=ba.group_outventory(outventory_file_weekly, np.timedelta64(1, "W"), start_datetime=Time("2008-10-27"), end_datetime=Time("2009-10-27"))
weekly_mosaic_list, weekly_total_mosaic=ba.parallel.batmosaic_analysis(batsurvey_obs, outventory_file_weekly, time_bins_weekly, nprocs=8)

weekly_mosaic_list=ba.parallel.batspectrum_analysis(weekly_mosaic_list, object_name, recalc=True, nprocs=11)
weekly_total_mosaic=ba.parallel.batspectrum_analysis(weekly_total_mosaic, object_name, recalc=True, use_cstat=False, nprocs=1)

#monthly mosaic (monthly averages) 
outventory_file_monthly=ba.merge_outventory(batsurvey_obs, savedir=Path('./3C273data/monthly_mosaiced_surveyresults/', reload=True))
time_bins_monthly=ba.group_outventory(outventory_file_monthly, np.timedelta64(1, "M"), start_datetime=Time("2008-10-27"), end_datetime=Time("2009-10-27"))
monthly_mosaic_list, monthly_total_mosaic=ba.parallel.batmosaic_analysis(batsurvey_obs, outventory_file_monthly, time_bins_monthly, nprocs=8)

monthly_mosaic_list=ba.parallel.batspectrum_analysis(monthly_mosaic_list, object_name, recalc=True, nprocs=11)
monthly_total_mosaic=ba.parallel.batspectrum_analysis(monthly_total_mosaic, object_name, recalc=True, use_cstat=False, nprocs=1)

#save our survey/mosaic results, here, begin to specify desired time unit (markian julian day, mjd here)  
all_data=ba.concatenate_data(batsurvey_obs, object_name, ["mjd_time", "utc_time", "exposure", "rate","rate_err","snr", "flux", "PhoIndex"])
with open('all_data_dictionary.pkl', 'wb') as f:
    pickle.dump(all_data, f)

all_data_monthly=ba.concatenate_data(mosaic_list, object_name, ["user_timebin/mjd_time", "user_timebin/utc_time", "user_timebin/mjd_stop_time", "user_timebin/utc_stop_time", "rate","rate_err","snr", "flux", "PhoIndex"])
with open('monthly_mosaic_dictionary.pkl', 'wb') as f:
    pickle.dump(all_data_monthly, f)

#If the weekly analysis has been completed, the next few lines can be uncommented:
all_data_weekly=ba.concatenate_data(weekly_mosaic_list, object_name, ["user_timebin/mjd_time", "user_timebin/utc_time", "user_timebin/mjd_stop_time", "user_timebin/utc_stop_time", "rate","rate_err","snr", "flux", "PhoIndex"])
with open('weekly_mosaic_dictionary.pkl', 'wb') as f:
    pickle.dump(all_data_weekly, f)

#create the main plot of your object in the manuscript, keeping consistent time unit 
energy_range=None
time_unit="MJD"
values=["rate","snr", "flux", "PhoIndex"]
survey_obsid_list=["all_data_dictionary","monthly_mosaic_dictionary", "weekly_mosaic_dictionary"]

obs_list_count=0
for observation_list in survey_obsid_list:

    with open(observation_list+".pkl", 'rb') as f:
        all_data=pickle.load(f)
        data=all_data[object_name]

    # get the time centers and errors
    if "mosaic" in observation_list:

        if "MET" in time_unit:
            t0 = TimeDelta(data["user_timebin/met_time"], format='sec')
            tf = TimeDelta(data["user_timebin/met_stop_time"], format='sec')
        elif "MJD" in time_unit:
            t0 = Time(data["user_timebin/mjd_time"], format='mjd')
            tf = Time(data["user_timebin/mjd_stop_time"], format='mjd')
        else:
            t0 = Time(data["user_timebin/utc_time"])
            tf = Time(data["user_timebin/utc_stop_time"])
    else:
        if "MET" in time_unit:
            t0 = TimeDelta(data["met_time"], format='sec')
        elif "MJD" in time_unit:
            t0 = Time(data["mjd_time"], format='mjd')
        else:
            t0 = Time(data["utc_time"])
        tf = t0 + TimeDelta(data["exposure"], format='sec')

    dt = tf - t0

    if "MET" in time_unit:
        time_center = 0.5 * (tf + t0).value
        time_diff = 0.5 * (tf - t0).value
    elif "MJD" in time_unit:
        time_diff = 0.5 * (tf - t0)
        time_center = t0 + time_diff
        time_center = time_center.value
        time_diff = time_diff.value

    else:
        time_diff = TimeDelta(0.5 * dt)  # dt.to_value('datetime')
        time_center = t0 + time_diff

        time_center = np.array([i.to_value('datetime64') for i in time_center])
        time_diff = np.array([np.timedelta64(0.5 * i.to_datetime()) for i in dt])

    x = time_center
    xerr = time_diff

    if obs_list_count == 0:
        fig, axes = plt.subplots(len(values), sharex=True, figsize=(10,12))

    axes_queue = [i for i in range(len(values))]
    # plot_value=[i for i in values]

    e_range_str = f"{14}-{195} keV"
    #axes[0].set_title(object_name + '; survey data from ' + e_range_str)

    for i in values:
        ax = axes[axes_queue[0]]
        axes_queue.pop(0)

        y = data[i]
        yerr = np.zeros(x.size)
        y_upperlim = np.zeros(x.size)

        label = i

        if "rate" in i:
            yerr = data[i + "_err"]
            label = "Count rate (cts/s)"
        elif i + "_lolim" in data.keys():
            # get the errors
            lolim = data[i + "_lolim"]
            hilim = data[i + "_hilim"]

            yerr = np.array([lolim, hilim])
            y_upperlim = data[i + "_upperlim"]

            # find where we have upper limits and set the error to 1 since the nan error value isnt
            # compatible with upperlimits
            yerr[:, y_upperlim] = 0.1 * y[y_upperlim]

        if "mosaic" in observation_list:
            if "weekly" in observation_list:
                zorder = 9
                c = "blue"
                m = "o"
                l="Weekly Mosaic"
                ms=5
                a=0.8
            else:
                zorder = 9
                c="green"
                m = "s"
                l = "Monthly Mosaic"
                ms=7
                a = 1
        else:
            zorder = 4
            c = "gray"
            m = "."
            l = "Survey Snapshot"
            ms=3
            a = 0.3

        ax.errorbar(x, y, xerr=xerr, yerr=yerr, uplims=y_upperlim, linestyle="None", marker=m, markersize=ms,
                    zorder=zorder, color=c, label=l, alpha=a)

        if ("flux" in i.lower()):
            ax.set_yscale('log')

        if ("snr" in i.lower()):
            ax.set_yscale('log')

        ax.set_ylabel(label)

    # if T0==0:
    if "MET" in time_unit:
        label_string = 'MET Time (s)'
    elif "MJD" in time_unit:
        label_string = 'MJD Time (s)'
    else:
        label_string = 'UTC Time (s)'

    plt.gca().ticklabel_format(useMathText=True)
    axes[-1].set_xlabel(label_string)

    obs_list_count += 1

#add the UTC times as well
#met_values=[i.get_position()[0] for i in axes[-1].get_xticklabels()]
#utc_values=[np.datetime64(sbu.met2datetime(i)) for i in met_values]

#for i,j in zip(met_values, [2008, 2011]):
    #for ax in axes:
        #ax.axvline(i, 0, 1, ls='--', color='k')
        #if ax==axes[0]:
            #ax.text(i, ax.get_ylim()[1]*1.01, str(j), fontsize=12, ha='center')

#add the UTC times as well
utc_time=Time(["2008-12-15", "2010-10-27"])
met_time=[i.get_position()[0] for i in axes[-1].get_xticklabels()]
for i in utc_time:
    met_time.append(sbu.datetime2met(i.datetime, correct=True))

#for i,j in zip(met_time, utc_time.ymdhms):
    #for ax in axes:
        #ax.axvline(i, 0, 1, ls='--', color='k')
        #if ax==axes[0]:
            #ax.text(i, ax.get_ylim()[1]*1.03, f'{j["year"]}', fontsize=10, ha='center')            
            
axes[0].legend(loc="best")

axes[1].set_ylabel("SNR")
axes[2].set_ylabel(r"Flux (erg/s/cm$^2$)")
axes[3].set_ylabel(r"$\Gamma$")

for ax, l in zip(axes, ["a","b","c","d"]):
    ax.text(.99, .95, f"({l})", ha='right', va='top', transform=ax.transAxes,  fontsize=12)

axes[-1].axhline(2.15, 0, 1)

axes[-2].axhline(23342.70e-12, 0, 1)

fig.tight_layout()
plot_filename = object_name + '_survey_lc.pdf'
fig.savefig(plot_filename, bbox_inches="tight")
