#!/usr/bin/env python
# coding: utf-8

# In[1]:


import glob
import os
import sys
import batanalysis as ba
import matplotlib.pyplot as plt
import numpy as np
from astropy.time import Time, TimeDelta
from astropy.io import fits
from pathlib import Path
import swiftbat
import swiftbat.swutil as sbu
import pickle


# In[2]:


plt.ion()


# In[3]:


#set the path with the BAT survey data
newdir = Path("/home/bmead/data/bat/Mrk501data")
ba.datadir(newdir, mkdir=True)


# In[4]:


#query heasarc for all the data within the time period of interest and download it
object_name='Mrk501'
queryargs = dict(time="2008-10-27 .. 2009-10-27", fields='All', resultmax=0)


# In[5]:


#use swiftbat to create a bat source object
object_location = swiftbat.simbadlocation("Mrk501")
object_batsource = swiftbat.source(ra=object_location[0], dec=object_location[1], name=object_name)
table_everything = ba.from_heasarc(name=None, **queryargs)
minexposure = 2000     # cm^2 after cos adjust


# In[6]:


#calculate the exposure with partial coding
exposures = np.array([object_batsource.exposure(ra=row['RA'], dec=row['DEC'], roll=row['ROLL_ANGLE'])[0] for row in table_everything])


# In[7]:


#select the observations that have greater than the minimum desired exposure
table_exposed = table_everything[exposures > minexposure]
print(f"Finding everything finds {len(table_everything)} observations, of which {len(table_exposed)} have more than {minexposure:0} cm^2 coded")


# In[8]:


#download the data
result = ba.download_swiftdata(table_exposed[:2],save_dir="/home/bmead/data/bat/Mrk501data",reload='True')


# In[9]:


table_exposed


# In[10]:


obs_ids=[i for i in table_exposed[:2]['OBSID'] if result[i]['success']]


# In[11]:


input_dict=dict(cleansnr=6,cleanexpr='ALWAYS_CLEAN==T')
noise_map_dir=Path("/Users/bmead/data/bat/PATTERN_MAPS/")
batsurvey_obs=ba.parallel.batsurvey_analysis(obs_ids, input_dict=input_dict, patt_noise_dir=noise_map_dir, nprocs=20)


# In[12]:


batsurvey_obs=ba.parallel.batspectrum_analysis(batsurvey_obs, object_name, ul_pl_index=2.15, recalc=True,nprocs=14)


# In[13]:


#plot the snapshot pointing values of rate, snr, and the fitted flux and photon index
fig, axes=ba.plot_survey_lc(batsurvey_obs, id_list=object_name, time_unit="UTC", values=["rate","snr", "flux", "PhoIndex", "exposure"], calc_lc=True)


# In[14]:


#when loading information:
#outventory_file=Path("/bat/3C273data/mosaiced_surveyresults/grouped_outventory/")
#mosaicing analysis
#group together all the BAT survey observations that we want to include in this step in our analysis
outventory_file=ba.merge_outventory(batsurvey_obs)


# In[15]:


#to define the time bins for which we will create mosaic images and analyze them, to test our code, we will use the same 1 month binning as the 22 month survey paper used
time_bins=ba.group_outventory(outventory_file, np.timedelta64(1, "M"), end_datetime=Time("2009-10-27"), recalc=True)


# In[ ]:


mosaic_list, total_mosaic=ba.parallel.batmosaic_analysis(batsurvey_obs, outventory_file, time_bins, nprocs=8)


# In[ ]:


mosaic_list=ba.parallel.batspectrum_analysis(mosaic_list, object_name, ul_pl_index=1.9, use_cstat=True, nprocs=5)
total_mosaic=ba.parallel.batspectrum_analysis(total_mosaic, object_name, ul_pl_index=1.9, use_cstat=True, nprocs=1)


# In[ ]:


fig, axes=ba.plot_survey_lc(mosaic_list, id_list=object_name, time_unit="UTC", values=["rate","snr", "flux", "PhoIndex", "exposure"])
fig, axes=ba.plot_survey_lc([batsurvey_obs,mosaic_list], id_list=object_name, time_unit="UTC", values=["rate","snr", "flux", "PhoIndex", "exposure"], same_figure=True)


# In[ ]:


#do the monthly mosaic analysis: we would do:
#outventory_file_weekly=ba.merge_outventory(batsurvey_obs, savedir=Path('./weekly_mosaiced_surveyresults/'))
outventory_file_monthly=ba.merge_outventory(batsurvey_obs, savedir=Path('./Mrk501data/monthly_mosaiced_surveyresults/', reload=True))
time_bins_monthly=ba.group_outventory(outventory_file_monthly, np.timedelta64(1, "M"), start_datetime=Time("2008-10-27"), end_datetime=Time("2009-10-27"))
monthly_mosaic_list, monthly_total_mosaic=ba.parallel.batmosaic_analysis(batsurvey_obs, outventory_file_monthly, time_bins_monthly, nprocs=8)

monthly_mosaic_list=ba.parallel.batspectrum_analysis(monthly_mosaic_list, object_name, recalc=True, nprocs=11)
monthly_total_mosaic=ba.parallel.batspectrum_analysis(monthly_total_mosaic, object_name, recalc=True, use_cstat=False, nprocs=1)


# In[ ]:


#save our survey/mosaic results
all_data=ba.concatenate_data(batsurvey_obs,'Mrk501', ["mjd_time", "rate", "rate_err", "obs_ids","pointing_id", "flux"])
with open('all_data_dictionary.pkl', 'wb') as f:
    pickle.dump(all_data, f)


# In[ ]:


import pandas as pd
df = pd.DataFrame.from_dict(all_data, orient='columns', dtype=None, columns=None)
print(df)
#try changing this section to orient rows 


# In[ ]:


df = df.drop(["flux_hilim","flux_lolim","flux_upperlim"])


# In[ ]:


df.dtypes


# In[ ]:


df.to_csv('Mrk501Database.csv', sep='\t', index=False,header=True)


# In[ ]:
